{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import exceptions\n",
    "import dotenv\n",
    "import os\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "base_url = 'https://au.indeed.com/jobs?q=data+engineer&l=Australia&sort=date'\n",
    "job_url = 'https://au.indeed.com/viewjob?jk='\n",
    "\n",
    "urllib3.disable_warnings()\n",
    "dotenv.load_dotenv()\n",
    "proxy_list = [\"28a1cac97213e66ca5d0d04cefce05c5\",\"28a1cac97213e66ca5d0d04cefce05c5\"]\n",
    "\n",
    "def fetch_html(url):\n",
    "    while True:\n",
    "        try:\n",
    "            proxy = {\"https\": f'http://scraperapi:{proxy_list[0]}@proxy-server.scraperapi.com:8001'}\n",
    "            print(url)\n",
    "            page = requests.get(url, verify=False, proxies=proxy, timeout=60)\n",
    "            print(page.status_code)\n",
    "            if page.status_code == 403:\n",
    "                print('Maximum scraperAPI limit reached, switching keys')\n",
    "                proxy_list.pop(0)\n",
    "                continue\n",
    "            elif page.status_code == 429:\n",
    "                print('Maximum concurrency limit reached')\n",
    "                sys.exit()\n",
    "            page.raise_for_status()  # Raise exceptions if there are request problems\n",
    "            html = BeautifulSoup(page.content, 'html.parser')\n",
    "            return html\n",
    "        except IndexError:\n",
    "            print('All proxies exhausted.')\n",
    "            sys.exit()\n",
    "        except requests.exceptions.HTTPError:\n",
    "            print(page.status_code)\n",
    "            print('Skipping')\n",
    "            continue\n",
    "\n",
    "\n",
    "def get_job_links(num_jobs):\n",
    "        \"\"\"\n",
    "        Method used to generate the individual job links\n",
    "        Replaceable if js rendering is available.\n",
    "\n",
    "        :param num_jobs: number of job links to be returned\n",
    "        :return: _job_links(list)\n",
    "        \"\"\"\n",
    "        job_list = set()\n",
    "        query = ''\n",
    "        pages, remaining_jobs = divmod(num_jobs, 15)\n",
    "\n",
    "        for page in range(pages + 1):\n",
    "            try:\n",
    "                results = fetch_html(base_url + query).find(id='resultsCol')\n",
    "                print('Request for job list success..')\n",
    "                jobs = results.findAll(class_='jobsearch-SerpJobCard unifiedRow row result')\n",
    "                temp = remaining_jobs if page == pages else 15\n",
    "                for job in jobs[:temp]:\n",
    "                    job_id = job.attrs['data-jk']\n",
    "                    link = job_url + job_id\n",
    "                    job_list.append(link)\n",
    "                query = f'&start={page * 10}'  # page 2 on indeed is &start=10\n",
    "            except exceptions.HTTPError:\n",
    "                print('HTTP error, skipping job')\n",
    "                continue\n",
    "            except exceptions.ConnectionError:\n",
    "                print('Connection Error, skipping job')\n",
    "                continue\n",
    "            except exceptions.Timeout:\n",
    "                print('Requests timed out, try re-checking your links.')\n",
    "                continue\n",
    "\n",
    "        return list(job_list)\n",
    "\n",
    "\n",
    "def get_job_links_today():\n",
    "    \"\"\"\n",
    "    Grabs all job links that are posted today\n",
    "    Same as job_links method except it scrapes until there are no more jobs for today.\n",
    "\n",
    "    :return: job links in a list\n",
    "    \"\"\"\n",
    "    job_list = list()\n",
    "    query = ''\n",
    "    posted_today = True\n",
    "    page = 0\n",
    "    while posted_today:\n",
    "        try:\n",
    "            results = fetch_html(base_url + query).find(id='resultsCol')\n",
    "            jobs = results.findAll(class_='jobsearch-SerpJobCard unifiedRow row result')\n",
    "            for job in jobs:\n",
    "                try:\n",
    "                    date = job.find('span', class_='date').text\n",
    "                except AttributeError:\n",
    "                    continue\n",
    "\n",
    "                if date in ['Just posted', 'Today']:  # No numbers, i.e. today or just now posted.\n",
    "                    job_id = job.attrs['data-jk']\n",
    "                    link = job_url + job_id\n",
    "                    job_list.append(link)\n",
    "                else:\n",
    "                    print('No more jobs posted today')\n",
    "                    posted_today = False\n",
    "                    return job_list\n",
    "\n",
    "            page += 1\n",
    "            query = f'&start={page * 10}'  # page 2 on indeed is &start=10\n",
    "        except exceptions.HTTPError:\n",
    "            print('HTTP error, skipping job')\n",
    "            continue\n",
    "        except exceptions.ConnectionError:\n",
    "            print('Connection Error, skipping job')\n",
    "            continue\n",
    "        except exceptions.Timeout:\n",
    "            print('Requests timed out, try re-checking your links.')\n",
    "            continue\n",
    "\n",
    "\n",
    "def categorise_position(positions):\n",
    "    \"\"\"\n",
    "    Loops through list of position types and cleans word into specific categories.\n",
    "\n",
    "    :param positions: List of positions\n",
    "    :return: List of positions that's been cleaned\n",
    "    \"\"\"\n",
    "    kws = {'full': 'fullTime', 'part': 'partTime', 'intern': 'internship', 'permanent': 'permanent',\n",
    "           'casual': 'casual', 'sub': 'subContract', 'contract': 'contract', 'temp': 'temporary', 'fly': 'flyInOut'}\n",
    "    for kw in kws:\n",
    "        for index, word in enumerate(positions):\n",
    "            if kw in word.lower():\n",
    "                positions[index] = kws[kw]\n",
    "                continue\n",
    "    return positions\n",
    "\n",
    "\n",
    "def html_parser(link):\n",
    "        html = fetch_html(link)\n",
    "        print('Request success... scraping job.')\n",
    "        print(link)\n",
    "        job_container = html.find('div', class_='jobsearch-JobComponent')\n",
    "\n",
    "        # Get job offer id from the link\n",
    "        if link:\n",
    "            pattern = 'jk=.*'\n",
    "            job_offer_id = re.search(pattern, link).group()[3:]\n",
    "        else:\n",
    "            job_offer_id = None\n",
    "\n",
    "        # Get job offer title\n",
    "        try:\n",
    "            job_title = job_container.find('h1', class_='jobsearch-JobInfoHeader-title').text\n",
    "        except AttributeError:\n",
    "            job_title = None\n",
    "\n",
    "        # Get company name\n",
    "        try:\n",
    "            company = job_container.find('div', class_='icl-u-lg-mr--sm icl-u-xs-mr--xs').text\n",
    "        except AttributeError:\n",
    "            company = None\n",
    "\n",
    "        # Get company location\n",
    "        try:\n",
    "            locations = job_container.find('div', class_='icl-u-xs-mt--xs icl-u-textColor--secondary jobsearch-JobInfoHeader-subtitle '\n",
    "                                                         'jobsearch-DesktopStickyContainer-subtitle').select('div')[-1].text\n",
    "            states = ['VIC', 'NSW', 'SA', 'QLD', 'TAS', 'WA', 'ACT', 'NT']\n",
    "\n",
    "            location = locations.split()\n",
    "            postcode = None\n",
    "            state = None\n",
    "            suburb = ''\n",
    "            for i in location:\n",
    "                if re.search(r'\\d', i):\n",
    "                    postcode = i\n",
    "                elif i in states:\n",
    "                    state = i\n",
    "                else:\n",
    "                    suburb += i + ' '\n",
    "            suburb = suburb.strip()\n",
    "\n",
    "        except AttributeError:\n",
    "            suburb = None\n",
    "            state = None\n",
    "            postcode = None\n",
    "\n",
    "        except IndexError:\n",
    "            suburb = None\n",
    "            state = None\n",
    "            postcode = None\n",
    "\n",
    "        # Grabbing salary and position_status\n",
    "        try:\n",
    "            sal_and_pos = job_container.find('div', class_='jobsearch-JobMetadataHeader-item').findAll('span')\n",
    "            # If there are two elements , grab both.\n",
    "            if len(sal_and_pos) == 2:\n",
    "                salary = sal_and_pos[0].text\n",
    "                position_status = sal_and_pos[1].text.replace(u'\\xa0', '')\n",
    "                # Hyphen only exists when there are two elements.\n",
    "                pattern = '-.*?'\n",
    "                position_status = re.sub(pattern, '', position_status, count=1).replace(' ', '').split(',')\n",
    "            # Otherwise, if the single element contains digits, assign to salary.\n",
    "            elif bool(re.search(r'\\d', sal_and_pos[0].text)):\n",
    "                salary = sal_and_pos[0].text\n",
    "                position_status = []\n",
    "            # If no digits, assign to position status instead.\n",
    "            else:\n",
    "                salary = None\n",
    "\n",
    "                position_status = sal_and_pos[0].text.replace(u'\\xa0', '').replace(' ', '').split(',')\n",
    "        # When salary & position doesn't exist\n",
    "        except AttributeError:\n",
    "            salary = None\n",
    "            position_status = []\n",
    "\n",
    "        if position_status:\n",
    "            position_status = categorise_position(position_status)\n",
    "\n",
    "        # Get company logo src\n",
    "        try:\n",
    "            logo_container = html.find('img', class_='jobsearch-CompanyAvatar-image')\n",
    "            company_logo = logo_container['src']\n",
    "            logo_alt = logo_container['alt']\n",
    "            logo = {'src': company_logo, 'alt': logo_alt}\n",
    "\n",
    "        # Company doesn't have logo uploaded\n",
    "        except TypeError:\n",
    "            logo = None\n",
    "\n",
    "        # Grab date posted epoch\n",
    "        try:\n",
    "            date_posted = job_container.find('div', class_='jobsearch-JobMetadataFooter').text\n",
    "            temp = re.findall(r'\\d+', date_posted)  # Check if number exists\n",
    "            # If number doesn't exist, job is posted today.\n",
    "            date_posted = datetime.today().timestamp() if not temp else \\\n",
    "                (datetime.today() - timedelta(days=int(temp[0]))).timestamp()\n",
    "            date_posted = round(date_posted)\n",
    "        except AttributeError:\n",
    "            date_posted = None\n",
    "\n",
    "        # Grab job description\n",
    "        try:\n",
    "            job_description = job_container.find('div', id='jobDescriptionText').text\n",
    "        except AttributeError:\n",
    "            job_description = None\n",
    "\n",
    "        # job_offer = self._job_output_format(job_title, job_offer_id, company, position_status, salary, date_posted,\n",
    "        #                                     job_description, suburb, state, link, self.reference, l_src=)\n",
    "\n",
    "        job_offer = {\n",
    "            'title': job_title,\n",
    "            'id': job_offer_id,\n",
    "            'externalId': None,\n",
    "            'workingVisa': None,\n",
    "            'visaSubclass': None,\n",
    "            'company': {\n",
    "                'name': company,\n",
    "                'id': '1',  # Our end unique company id\n",
    "                'logo': logo,\n",
    "            },\n",
    "            'positions': position_status,\n",
    "            'salary': salary,\n",
    "            'datePosted': date_posted,\n",
    "            'dateExpiring': None,\n",
    "            'description': job_description,\n",
    "            'location': {\n",
    "                'suburb': suburb,\n",
    "                'state': state,\n",
    "                'postcode': postcode,\n",
    "                'description': None\n",
    "            },\n",
    "            'link': link,\n",
    "            'reference': 'indeed',\n",
    "            'activeStatus': True,\n",
    "            'categories': []\n",
    "        }\n",
    "\n",
    "        return job_offer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://au.indeed.com/jobs?q=data+engineer&l=Australia&sort=date\n",
      "200\n",
      "https://au.indeed.com/jobs?q=data+engineer&l=Australia&sort=date&start=10\n",
      "200\n",
      "No more jobs posted today\n"
     ]
    }
   ],
   "source": [
    "jobs_links = get_job_links_today()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://au.indeed.com/viewjob?jk=de8810e65eead281\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=de8810e65eead281\n",
      "https://au.indeed.com/viewjob?jk=b133955982d37e45\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=b133955982d37e45\n",
      "https://au.indeed.com/viewjob?jk=062bd611f9984bc6\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=062bd611f9984bc6\n",
      "https://au.indeed.com/viewjob?jk=a1271c98dffb5122\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=a1271c98dffb5122\n",
      "https://au.indeed.com/viewjob?jk=4f145b74137e3386\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=4f145b74137e3386\n",
      "https://au.indeed.com/viewjob?jk=2145d2b5bdd72dcb\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=2145d2b5bdd72dcb\n",
      "https://au.indeed.com/viewjob?jk=9751a20726a9c915\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=9751a20726a9c915\n",
      "https://au.indeed.com/viewjob?jk=58c6b9f7879d8650\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=58c6b9f7879d8650\n",
      "https://au.indeed.com/viewjob?jk=b04d6f83201e2284\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=b04d6f83201e2284\n",
      "https://au.indeed.com/viewjob?jk=748c036effea61ac\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=748c036effea61ac\n",
      "https://au.indeed.com/viewjob?jk=2242102227d931a5\n",
      "200\n",
      "Request success... scraping job.\n",
      "https://au.indeed.com/viewjob?jk=2242102227d931a5\n"
     ]
    }
   ],
   "source": [
    "job_links_data = [] \n",
    "\n",
    "for link in jobs_links:\n",
    "    job_links_data.append(html_parser(link))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing jobs to job_data..\n"
     ]
    }
   ],
   "source": [
    "print('Writing jobs to job_data..')\n",
    "fname = f\"data/output/job_data_{datetime.today().strftime('%Y_%m_%d')}.json\"\n",
    "with open(fname, \"w\") as write_file:\n",
    "    json.dump(job_links_data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_visa_filter(filename):\n",
    "    \"\"\"\n",
    "    Loads json object given in a specific format and filters the job by its description.\n",
    "\n",
    "    Keywords used:\n",
    "        kw_visa -> Positively connotated words for working visa.\n",
    "        temp_visa -> Neutral keywords that mention working/temporary visa.\n",
    "    filtered_jobs: Jobs are stored in a nested dictionary, separated in a list.\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'r') as rf:\n",
    "        data = json.load(rf)\n",
    "\n",
    "\n",
    "    kw_visa = ['valid temporary visa', 'valid work permit', '482 ', 'valid visa', 'appropriate visa',\n",
    "               'current visa', 'temporary visa holder may only occur if no suitable', 'working holiday visa accepted',\n",
    "               'visa sponsorship support']\n",
    "\n",
    "    temp_visa = ['temporary visa', 'work visa', 'working visa', 'australian visa']\n",
    "\n",
    "\n",
    "    for job in data:\n",
    "        if any(word in job['description'].lower() for word in kw_visa):\n",
    "            job['workingVisa'] = True\n",
    "        elif any(word in job['description'].lower() for word in temp_visa):\n",
    "            job['workingVisa'] = None\n",
    "        else:\n",
    "            job['workingVisa'] = False\n",
    "    \n",
    "    filtered_fname = f\"filtered_{filename}\"\n",
    "    with open(filtered_fname, 'w') as wf:\n",
    "        print('Writing filtered jobs to filteredJobs..')\n",
    "        json.dump(data, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing filtered jobs to filteredJobs..\n"
     ]
    }
   ],
   "source": [
    "job_visa_filter(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
